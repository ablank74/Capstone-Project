{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c8400ad",
   "metadata": {},
   "source": [
    "Here we will work on gathering the data using the Jira API to build our CSV with a larger dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efe1d28-bc48-4832-9e01-80c249ed3bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from dotenv import load_dotenv  # Add python-dotenv for better env management\n",
    "\n",
    "load_dotenv\n",
    "BEARER_TOKEN = os.getenv('JIRA_BEARER_TOKEN')\n",
    "JIRA_URL = 'https://csc.cioxhealth.com'\n",
    " \n",
    "session = requests.Session()\n",
    "session.headers.update({\n",
    "    'Authorization': f'Bearer {BEARER_TOKEN}',\n",
    "    'Content-Type': 'application/json'  # Ensure Content-Type is set to application/json\n",
    "    })\n",
    "\n",
    "def login_to_jira():\n",
    "    response = session.get(JIRA_URL + '/rest/api/2/user?username=SVC_IT_Automation')\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        print(\"Successfully logged in to Jira\")\n",
    "        #print(response.json())\n",
    "        return True\n",
    "    else:\n",
    "        print(\"Failed to log in to Jira\")\n",
    "        print(response.status_code, response.text)\n",
    "        return False\n",
    "\n",
    "\n",
    "# Login to JIRA\n",
    "if not login_to_jira():\n",
    "    raise Exception(\"Failed to log in to JIRA\")\n",
    "\n",
    "# Data retrieval variables\n",
    "start_at = 0\n",
    "max_results = 100\n",
    "total = None\n",
    "fields = ['key', 'project', 'summary', 'description', 'assignee', 'httpUrl', 'customfield_19900', 'customfield_15404', 'customfield_14201']\n",
    "issues_list = []\n",
    "\n",
    "# Prepare for progress bar\n",
    "pbar = tqdm(total=None)  # Total will be updated after first response\n",
    "\n",
    "# Main loop for data retrieval\n",
    "while total is None or start_at < total:\n",
    "    search_data = {\n",
    "        'jql': \"project='29-IT Service Desk' AND status=closed AND created >= startOfYear(-6m)\",\n",
    "        'fields': fields,\n",
    "        'startAt': start_at,\n",
    "        'maxResults': max_results\n",
    "    }\n",
    "\n",
    "    search_response = session.post(\n",
    "        f\"{JIRA_URL}/rest/api/2/search\",\n",
    "        data=json.dumps(search_data),\n",
    "        timeout=300\n",
    "    )\n",
    "\n",
    "    if search_response.status_code == 200:\n",
    "        search_result = search_response.json()\n",
    "        if 'issues' in search_result:\n",
    "            issues_list.extend(search_result['issues'])\n",
    "\n",
    "        if total is None:\n",
    "            total = search_result['total']\n",
    "            pbar.total = total // max_results + (total % max_results > 0)\n",
    "            pbar.refresh()\n",
    "\n",
    "        start_at += max_results\n",
    "        pbar.update(1)\n",
    "\n",
    "    elif search_response.status_code == 401:\n",
    "        if not login_to_jira():\n",
    "            print(\"Failed to re-authenticate.\")\n",
    "            break\n",
    "\n",
    "    else:\n",
    "        print(\"Received unexpected status code:\", search_response.status_code)\n",
    "        print(search_response.text)\n",
    "        break\n",
    "\n",
    "pbar.close()\n",
    "\n",
    "# Convert collected data to DataFrame\n",
    "df = pd.json_normalize(issues_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ce61cb",
   "metadata": {},
   "source": [
    "We'll save the dataframe to a CSV so that we have a saved copy incase something happens so we don't have to redownload it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "850de034",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "# Save DataFrame to CSV\n",
    "now = datetime.datetime.now()\n",
    "df.to_csv(f'dataframe{now:%Y-%m-%d_%H-%M}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4fa09a",
   "metadata": {},
   "source": [
    "Lets look at the size and a sample of our dataframe we'll be working with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b45705",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = len(df)\n",
    "print(f\"Dataframe Size: {size:.2f}KB\")\n",
    "\n",
    "print(df.columns)\n",
    "print(df.sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c22350c",
   "metadata": {},
   "source": [
    "Verify GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ab1d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    # Set device to the first CUDA device\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(f\"Using device: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "    # Create random tensors\n",
    "    x = torch.randn(5000, 5000, device=device)\n",
    "    y = torch.randn(5000, 5000, device=device)\n",
    "\n",
    "    # Perform matrix multiplication\n",
    "    print(\"Performing matrix multiplication on GPU...\")\n",
    "    z = torch.matmul(x, y)\n",
    "\n",
    "    # The operation is performed on the GPU, so if this script runs without errors\n",
    "    # and you observe the GPU utilization go up in your system's resource monitor,\n",
    "    # then the Nvidia A1000 is being used.\n",
    "    print(\"Done with matrix multiplication.\")\n",
    "\n",
    "else:\n",
    "    print(\"CUDA is not available. Using CPU instead.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0639e5",
   "metadata": {},
   "source": [
    "This cell is used to reload the dataframe from the CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8756efc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Load CSV\n",
    "df = pd.read_csv('dataframe2024-11-21_04-29.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0be2186d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unneeded columns\n",
    "df = df[[\n",
    "    'fields.customfield_14201',\n",
    "    'fields.assignee.displayName',\n",
    "    'fields.customfield_15404.value',\n",
    "    'fields.summary',\n",
    "    'fields.description'\n",
    "]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c209fb1",
   "metadata": {},
   "source": [
    "We need to rename the columns to make it more easily understandable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4b0292",
   "metadata": {},
   "source": [
    "Rename columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1e49fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={\n",
    "    'fields.customfield_14201':'Category 1',\n",
    "    'fields.assignee.displayName':'Assignee',\n",
    "    'fields.customfield_15404.value':'IT Group',\n",
    "    'fields.summary':'Summary',\n",
    "    'fields.description':'Description'\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797f6435",
   "metadata": {},
   "source": [
    "Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ad14c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "# Download only stopwords if needed\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    # Remove stopwords - using simple split() instead of word_tokenize\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = text.split()\n",
    "    text = ' '.join([w for w in words if w not in stop_words])\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Clean Summary and Description fields\n",
    "df['Summary'] = df['Summary'].apply(clean_text)\n",
    "df['Description'] = df['Description'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23dc86cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)\n",
    "print(df.sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0c6053",
   "metadata": {},
   "source": [
    "This is to clean up the code, removing all NaN rows as well as all ITSD rows so only properly assigned tickets are in place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "720547f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop all rows with IT Group = NaN\n",
    "df = df.dropna(subset=['IT Group'])\n",
    "\n",
    "#drop all ITSD tickets\n",
    "df = df[df['IT Group'] != 'ITSD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b43cd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['IT Group'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865f3ae5",
   "metadata": {},
   "source": [
    "Analyze Class Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db358a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze class distribution\n",
    "print(\"Class distribution:\")\n",
    "print(df['IT Group'].value_counts())\n",
    "print(\"\\nPercentage distribution:\")\n",
    "print(df['IT Group'].value_counts(normalize=True) * 100)\n",
    "\n",
    "# Visualize the distribution\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "df['IT Group'].value_counts().plot(kind='bar')\n",
    "plt.title('Distribution of IT Groups')\n",
    "plt.xlabel('IT Group')\n",
    "plt.ylabel('Number of Tickets')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca1e05f",
   "metadata": {},
   "source": [
    "This reduces the dataset to be 20% of the original to fit within memory constraints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81282851",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#reducing size of dataset\n",
    "df = df.sample(frac=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe17044",
   "metadata": {},
   "source": [
    "Now we start our work.  We load the dataframe from the CSV, and we'll start filling in empty fields to prevent errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89951f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "import gc\n",
    "\n",
    "# Combine 'Summary', 'Description', and 'Last Comment Assignee' into one field\n",
    "#df['combined'] = df[['Summary', 'Description', 'Last Comment Assignee']].fillna('').agg(' '.join, axis=1)\n",
    "df['combined'] = df[['Summary', 'Description']].fillna('').agg(' '.join, axis=1)\n",
    "\n",
    "# Drop the original fields\n",
    "#df = df.drop(['Summary', 'Description', 'Last Comment Assignee'], axis=1)\n",
    "#df = df.drop(['Summary', 'Description'], axis=1)\n",
    "\n",
    "# Fill in the blank values in 'IT Group', 'Assignee', 'Category 1', 'Category 2', and 'Category 3'\n",
    "imputer = SimpleImputer(strategy='constant', fill_value='unknown')\n",
    "#df[['IT Group', 'Assignee', 'Category 1', 'Category 2', 'Category 3']] = imputer.fit_transform(df[['IT Group', 'Assignee', 'Category 1', 'Category 2', 'Category 3']])\n",
    "df[['IT Group', 'Assignee', 'Category 1']] = imputer.fit_transform(df[['IT Group', 'Assignee', 'Category 1']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6aaa764",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Convert data types for memory efficiency\n",
    "# df = df.astype({'col_name': 'category', 'int_col': 'int32', ...})\n",
    "\n",
    "\n",
    "\n",
    "# Create and fit label encoders\n",
    "le_it_group = LabelEncoder()\n",
    "#le_assignee = LabelEncoder()\n",
    "df['IT Group'] = le_it_group.fit_transform(df['IT Group'])\n",
    "#df['Assignee'] = le_assignee.fit_transform(df['Assignee'])\n",
    "print(\"Label Encoders Completed\")\n",
    "\n",
    "# Drop original columns if they are no longer needed\n",
    "#df.drop(['IT Group', 'Assignee'], axis=1, inplace=True)\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['combined'], df[['IT Group', 'Assignee']], test_size=0.2, random_state=42)\n",
    "print(\"Train/Test split completed\")\n",
    "\n",
    "# Create a pipeline for each category\n",
    "pipeline_it_group = Pipeline([('tfidf', TfidfVectorizer(max_features=10000)), ('clf', RandomForestClassifier(n_jobs=-1))])\n",
    "#pipeline_assignee = Pipeline([('tfidf', TfidfVectorizer(max_features=10000)), ('clf', RandomForestClassifier(n_jobs=-1))])\n",
    "print(\"Pipelines created\")\n",
    "\n",
    "# Train the models\n",
    "pipeline_it_group.fit(X_train, y_train['IT Group'])\n",
    "#pipeline_assignee.fit(X_train, y_train['Assignee'])\n",
    "print(\"Training Completed\")\n",
    "\n",
    "# Clear memory\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7255bf",
   "metadata": {},
   "source": [
    "Addressing the IT Group class imbalance using SMOTE (Synthetic Minority Over-sampling Technique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafc730c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Keep our existing TF-IDF vectorizer\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=15000,\n",
    "    ngram_range=(1, 3),\n",
    "    min_df=2,\n",
    "    max_df=0.95,\n",
    "    strip_accents='unicode',\n",
    "    sublinear_tf=True\n",
    ")\n",
    "\n",
    "# Create base pipeline\n",
    "base_pipeline = Pipeline([\n",
    "    ('tfidf', tfidf),\n",
    "    ('clf', RandomForestClassifier(\n",
    "        n_jobs=-1,\n",
    "        class_weight='balanced',\n",
    "        random_state=42  # Add for reproducibility\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Expanded parameter grid\n",
    "param_grid = {\n",
    "    'clf__n_estimators': [200, 300, 400],\n",
    "    'clf__max_depth': [20, 30, 40],\n",
    "    'clf__min_samples_split': [2, 5],\n",
    "    'clf__min_samples_leaf': [1, 2],\n",
    "    'clf__max_features': ['sqrt', 'log2'],  # Add feature selection strategy\n",
    "    'clf__bootstrap': [True, False]  # Add bootstrap sampling option\n",
    "}\n",
    "\n",
    "# Create and train grid search\n",
    "grid_search = GridSearchCV(\n",
    "    base_pipeline,\n",
    "    param_grid,\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    "    scoring='accuracy'  # Explicitly set scoring metric\n",
    ")\n",
    "\n",
    "print(\"Starting grid search...\")\n",
    "grid_search.fit(X_train, y_train['IT Group'])\n",
    "\n",
    "print(\"\\nBest parameters:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation score:\", grid_search.best_score_)\n",
    "\n",
    "# Use the best model\n",
    "pipeline_it_group = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c836f26",
   "metadata": {},
   "source": [
    "Testing XGBoost for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "59e4f1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "# Create pipeline with SMOTE\n",
    "pipeline_it_group = ImbPipeline([\n",
    "    ('tfidf', tfidf),\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('clf', RandomForestClassifier(n_jobs=-1))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abff085",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "print(\"Starting XGBoost training...\")\n",
    "\n",
    "# Create XGBoost pipeline\n",
    "xgb_pipeline = Pipeline([\n",
    "    ('tfidf', tfidf),  # Using same TF-IDF vectorizer as RandomForest\n",
    "    ('clf', XGBClassifier(\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        tree_method='hist',  # Change from gpu_hist to hist\n",
    "        device='cuda',       # Add device parameter\n",
    "        eval_metric='mlogloss',\n",
    "        #scale_pos_weight=1  # For imbalanced classes\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Ensure the TF-IDF vectorizer is fitted before prediction\n",
    "tfidf.fit(X_train)  # Fit the vectorizer on training data\n",
    "\n",
    "# Parameter grid for XGBoost\n",
    "xgb_param_grid = {\n",
    "    'clf__n_estimators': [300, 400],     # Match best RF parameters\n",
    "    'clf__max_depth': [8, 10, 12],       # XGBoost typically uses smaller depths\n",
    "    'clf__learning_rate': [0.05, 0.1],\n",
    "    'clf__min_child_weight': [1, 3],\n",
    "    'clf__subsample': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Create and train grid search\n",
    "xgb_grid_search = GridSearchCV(\n",
    "    xgb_pipeline,\n",
    "    xgb_param_grid,\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    "    scoring='accuracy'\n",
    ")\n",
    "\n",
    "print(\"Starting XGBoost grid search...\")\n",
    "xgb_grid_search.fit(X_train, y_train['IT Group'])\n",
    "\n",
    "print(\"\\nBest XGBoost parameters:\", xgb_grid_search.best_params_)\n",
    "print(\"Best XGBoost cross-validation score:\", xgb_grid_search.best_score_)\n",
    "\n",
    "# Get predictions from both models\n",
    "y_pred_rf = pipeline_it_group.predict(X_test)\n",
    "y_pred_xgb = xgb_grid_search.best_estimator_.predict(X_test)\n",
    "\n",
    "# Compare accuracies\n",
    "print(\"\\nModel Accuracy Comparison:\")\n",
    "print(f\"RandomForest: {accuracy_score(y_test['IT Group'], y_pred_rf):.4f}\")\n",
    "print(f\"XGBoost:     {accuracy_score(y_test['IT Group'], y_pred_xgb):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5a1f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_it_group = pipeline_it_group.predict(X_test)\n",
    "#y_pred_assignee = pipeline_assignee.predict(X_test)\n",
    "#y_pred_category1 = pipeline_category1.predict(X_test)\n",
    "#y_pred_category2 = pipeline_category2.predict(X_test)\n",
    "#y_pred_category3 = pipeline_category3.predict(X_test)\n",
    "\n",
    "# Calculate accuracy scores\n",
    "accuracy_it_group = accuracy_score(y_test['IT Group'], y_pred_it_group)\n",
    "#accuracy_assignee = accuracy_score(y_test['Assignee'], y_pred_assignee)\n",
    "#accuracy_category1 = accuracy_score(y_test['Category 1'], y_pred_category1)\n",
    "#accuracy_category2 = accuracy_score(y_test['Category 2'], y_pred_category2)\n",
    "#accuracy_category3 = accuracy_score(y_test['Category 3'], y_pred_category3)\n",
    "\n",
    "print(f\"Accuracy of IT Group model: {accuracy_it_group}\")\n",
    "#print(f\"Accuracy of Assignee model: {accuracy_assignee}\")\n",
    "#print(f\"Accuracy of Category 1 model: {accuracy_category1}\")\n",
    "#print(f\"Accuracy of Category 2 model: {accuracy_category2}\")\n",
    "#print(f\"Accuracy of Category 3 model: {accuracy_category3}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "860933c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = pd.DataFrame({\n",
    "    'Summary': ['This is a new summary.'],\n",
    "    'Description': ['This is a new description.'],\n",
    "    'Last Comment Assignee': ['This is a new last comment.'],\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bf8b51",
   "metadata": {},
   "source": [
    "Evaluate model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa50f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with actual values\n",
    "df_actual = y_test.copy()\n",
    "\n",
    "# Convert encoded labels back to original labels\n",
    "df_actual['IT Group'] = le_it_group.inverse_transform(df_actual['IT Group'])\n",
    "#df_actual['Assignee'] = le_assignee.inverse_transform(df_actual['Assignee'])\n",
    "#df_actual['Category 1'] = le_category1.inverse_transform(df_actual['Category 1'])\n",
    "#df_actual['Category 2'] = le_category2.inverse_transform(df_actual['Category 2'])\n",
    "#df_actual['Category 3'] = le_category3.inverse_transform(df_actual['Category 3'])\n",
    "\n",
    "# Create a DataFrame with predicted values\n",
    "df_predicted = pd.DataFrame({\n",
    "    'IT Group': le_it_group.inverse_transform(y_pred_it_group),\n",
    "    #'Assignee': le_assignee.inverse_transform(y_pred_assignee),\n",
    "#    'Category 1': le_category1.inverse_transform(y_pred_category1),\n",
    "#    'Category 2': le_category2.inverse_transform(y_pred_category2),\n",
    "#    'Category 3': le_category3.inverse_transform(y_pred_category3),\n",
    "}, index=y_test.index)\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)  # or a large number instead of None\n",
    "pd.set_option('display.width', None)  # or a large number instead of None\n",
    "\n",
    "# Concatenate the actual and predicted values for comparison\n",
    "df_comparison = pd.concat([df_actual, df_predicted], axis=1)\n",
    "\n",
    "# Rename columns for clarity\n",
    "df_comparison.columns = ['IT Group (actual)', 'Assignee (actual)', 'IT Group (predicted)']\n",
    "\n",
    "# Display a sample\n",
    "print(df_comparison.sample(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd1d8ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, predict the IT Group\n",
    "predicted_group = pipeline_it_group.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4e1f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# First, predict the IT Group\n",
    "predicted_group = pipeline_it_group.predict(X_test)\n",
    "\n",
    "# Calculate accuracy for IT Group\n",
    "it_group_accuracy = accuracy_score(y_test['IT Group'], predicted_group)\n",
    "print(f\"IT Group Accuracy: {it_group_accuracy * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04092b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5TokenizerFast\n",
    "import torch\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
    "tokenizer = T5TokenizerFast.from_pretrained('t5-base')\n",
    "\n",
    "# Check if a GPU is available and if not, use a CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "# Move the model to the GPU if available\n",
    "model = model.to(device)\n",
    "\n",
    "# Function to generate summary\n",
    "def generate_summary(text):\n",
    "    # Prepend 'summarize: ' to the text (T5 model convention for summarization tasks)\n",
    "    text = \"summarize: \" + text\n",
    "\n",
    "    # Tokenize the text\n",
    "    inputs = tokenizer.encode(text, return_tensors='pt', max_length=512, truncation=True)\n",
    "\n",
    "    # Move the inputs to the GPU if available\n",
    "    inputs = inputs.to(device)\n",
    "\n",
    "    # Generate a summary\n",
    "    summary_ids = model.generate(inputs, max_length=100, length_penalty=5., num_beams=2)\n",
    "\n",
    "    # Decode the summary\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    return summary\n",
    "\n",
    "print(\"Filling NaN with empty strings\")\n",
    "# Fill NaN values with empty strings\n",
    "df['Summary'].fillna('', inplace=True)\n",
    "df['Description'].fillna('', inplace=True)\n",
    "\n",
    "# Combine 'Summary' and 'Description' fields and apply the function to generate summaries\n",
    "# Note: For demonstration purposes, we're only applying this to the first 10 rows using .head(10)\n",
    "# Remove .head(10) to apply the function to the entire DataFrame\n",
    "print(\"Combining Summary and Description\")\n",
    "df['combined'] = df[['Summary', 'Description']].head(10).agg(' '.join, axis=1)\n",
    "\n",
    "print(\"Converting 'combined' column to string\")\n",
    "# Convert the 'combined' column to string type\n",
    "df['combined'] = df['combined'].astype(str)\n",
    "\n",
    "print(\"generating summaries\")\n",
    "df['AI_Summary'] = df['combined'].apply(generate_summary)\n",
    "\n",
    "# Show a sample\n",
    "print(df[['Summary', 'Description', 'AI_Summary']].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056e4d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('dataframe.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a616f33",
   "metadata": {},
   "source": [
    "DO NOT PROCEED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89233437",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_jira_ticket(ticket_number):\n",
    "    if not login_to_jira():\n",
    "        raise Exception(\"Failed to log in to JIRA\")\n",
    "\n",
    "    search_data = {\n",
    "        'jql': f\"issue = {ticket_number}\",\n",
    "        'fields': ['summary', 'description', 'assignee', 'customfield_19900', 'customfield_15404', 'customfield_14201'],\n",
    "        'startAt': 0,\n",
    "        'maxResults': 1\n",
    "    }\n",
    "\n",
    "    search_response = session.post(\n",
    "        f\"{JIRA_URL}/rest/api/2/search\",\n",
    "        data=json.dumps(search_data),\n",
    "        timeout=300\n",
    "    )\n",
    "\n",
    "    if search_response.status_code == 200:\n",
    "        search_result = search_response.json()\n",
    "        return pd.json_normalize(search_result['issues'])\n",
    "    else:\n",
    "        print(\"Failed to retrieve the ticket:\", search_response.status_code)\n",
    "        print(search_response.text)\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43770e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_it_group(ticket_number):\n",
    "    df_ticket = search_jira_ticket(ticket_number)\n",
    "    if df_ticket is not None and not df_ticket.empty:\n",
    "        # Assuming you need 'summary' and 'description' to predict IT group\n",
    "        combined_field = df_ticket['fields.summary'] + ' ' + df_ticket['fields.description']\n",
    "        predicted_group_encoded = pipeline_it_group.predict(combined_field)\n",
    "        predicted_group = le_it_group.inverse_transform(predicted_group_encoded)\n",
    "        return predicted_group[0]\n",
    "    else:\n",
    "        return \"Ticket not found or error in retrieval\"\n",
    "\n",
    "# Example usage\n",
    "ticket_number = \"ITSD-602049\"\n",
    "predicted_group = predict_it_group(ticket_number)\n",
    "print(f\"Predicted IT Group for ticket {ticket_number}: {predicted_group}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04029b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_jira_unassigned_tickets_in_group(group_name):\n",
    "    if not login_to_jira():\n",
    "        raise Exception(\"Failed to log in to JIRA\")\n",
    "\n",
    "    # Modify the JQL query\n",
    "    search_data = {\n",
    "        'jql': f\"project='29-IT Service Desk' AND 'IT Group' = '{group_name}' AND status != closed\",\n",
    "        'fields': ['key', 'summary', 'description', 'assignee', 'customfield_19900', 'customfield_15404', 'customfield_14201'],\n",
    "        'startAt': 0,\n",
    "        'maxResults': 100  # You can adjust the maxResults as needed\n",
    "    }\n",
    "\n",
    "    search_response = session.post(\n",
    "        f\"{JIRA_URL}/rest/api/2/search\",\n",
    "        data=json.dumps(search_data),\n",
    "        timeout=300\n",
    "    )\n",
    "\n",
    "    if search_response.status_code == 200:\n",
    "        search_result = search_response.json()\n",
    "        #return pd.json_normalize(search_result['issues'])\n",
    "    else:\n",
    "        print(\"Failed to retrieve tickets:\", search_response.status_code)\n",
    "        print(search_response.text)\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "group_name = \"M365\"\n",
    "unassigned_tickets_df = search_jira_unassigned_tickets_in_group(group_name)\n",
    "print(unassigned_tickets_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ccc206",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_and_suggest_it_group(group_name):\n",
    "    df_tickets = search_jira_unassigned_tickets_in_group(group_name)\n",
    "    if df_tickets is not None and not df_tickets.empty:\n",
    "        # Assuming 'summary' and 'description' fields are used for prediction\n",
    "        combined_fields = df_tickets['fields.summary'] + ' ' + df_tickets['fields.description']\n",
    "\n",
    "        # Predict IT Group\n",
    "        predicted_groups_encoded = pipeline_it_group.predict(combined_fields)\n",
    "        predicted_groups = le_it_group.inverse_transform(predicted_groups_encoded)\n",
    "\n",
    "        # Compare and Suggest\n",
    "        suggestions = []\n",
    "        for index, row in df_tickets.iterrows():\n",
    "            predicted_group = predicted_groups[index]\n",
    "            if predicted_group != group_name:\n",
    "                suggestions.append({'Ticket': row['key'], 'Current Group': group_name, 'Suggested Group': predicted_group})\n",
    "        \n",
    "        return suggestions\n",
    "    else:\n",
    "        return \"No tickets found or error in retrieval\"\n",
    "\n",
    "# Example usage\n",
    "group_name = \"M365\"\n",
    "suggestions = validate_and_suggest_it_group(group_name)\n",
    "\n",
    "if isinstance(suggestions, list) and len(suggestions) > 0:\n",
    "    print(\"Suggestions for reassignment:\")\n",
    "    for suggestion in suggestions:\n",
    "        print(f\"Ticket {suggestion['Ticket']} should be in '{suggestion['Suggested Group']}' instead of '{suggestion['Current Group']}'\")\n",
    "else:\n",
    "    print(suggestions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe832fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_and_suggest_it_group_with_confidence(group_name, confidence_threshold=0.20):\n",
    "    df_tickets = search_jira_unassigned_tickets_in_group(group_name)\n",
    "    if df_tickets is not None and not df_tickets.empty:\n",
    "        # Assuming 'summary' and 'description' fields are used for prediction\n",
    "        # Fill NaN values with an empty string or some default text\n",
    "        df_tickets['fields.summary'].fillna('', inplace=True)\n",
    "        df_tickets['fields.description'].fillna('', inplace=True)\n",
    "\n",
    "        combined_fields = df_tickets['fields.summary'] + ' ' + df_tickets['fields.description']\n",
    "\n",
    "        # Get Predicted Probabilities\n",
    "        predicted_probs = pipeline_it_group.predict_proba(combined_fields)\n",
    "        predicted_groups_encoded = pipeline_it_group.predict(combined_fields)\n",
    "        predicted_groups = le_it_group.inverse_transform(predicted_groups_encoded)\n",
    "\n",
    "        # Extract the highest probability as the confidence score\n",
    "        confidence_scores = predicted_probs.max(axis=1)\n",
    "\n",
    "        # Base URL for JIRA tickets\n",
    "        jira_ticket_base_url = \"https://csc.cioxhealth.com/browse/\"\n",
    "\n",
    "        # Compare, Suggest and Include Confidence Score and URL\n",
    "        suggestions = []\n",
    "        for index, row in df_tickets.iterrows():\n",
    "            predicted_group = predicted_groups[index]\n",
    "            confidence_score = confidence_scores[index]\n",
    "\n",
    "            # Create a URL to the ticket\n",
    "            ticket_url = jira_ticket_base_url + row['key']\n",
    "\n",
    "            # Only suggest reassignment if confidence is greater than the threshold\n",
    "            if confidence_score > confidence_threshold and predicted_group != group_name:\n",
    "                suggestions.append({\n",
    "                    'Ticket': f\"[{row['key']}]({ticket_url})\",\n",
    "                    'Current Group': group_name,\n",
    "                    'Suggested Group': predicted_group,\n",
    "                    'Confidence Score': confidence_score\n",
    "                })\n",
    "\n",
    "        # Sort suggestions by confidence score in descending order\n",
    "        sorted_suggestions = sorted(suggestions, key=lambda x: x['Confidence Score'], reverse=True)\n",
    "        \n",
    "        return sorted_suggestions\n",
    "    else:\n",
    "        return \"No tickets found or error in retrieval\"\n",
    "\n",
    "# Example usage\n",
    "group_name = \"M365\"\n",
    "suggestions = validate_and_suggest_it_group_with_confidence(group_name)\n",
    "\n",
    "if isinstance(suggestions, list) and len(suggestions) > 0:\n",
    "    print(\"Suggestions for reassignment with confidence scores and links to JIRA, sorted by confidence:\")\n",
    "    for suggestion in suggestions:\n",
    "        print(f\"Ticket {suggestion['Ticket']} should be in '{suggestion['Suggested Group']}' instead of '{suggestion['Current Group']}' with confidence score: {suggestion['Confidence Score']:.2f}\")\n",
    "else:\n",
    "    print(suggestions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacd6f14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
